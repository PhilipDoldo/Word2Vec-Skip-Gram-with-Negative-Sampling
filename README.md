# Word2Vec-Skip-Gram-with-Negative-Sampling

This code naively implements Word2Vec word embeddings from scratch using a skip-gram model with negative sampling. Minibatch gradient descent was applied to the negative log likelihoods of .. can I write with latex? $\int_{0}^{\infty} x^2 dx$ and $P(\text{word} = A)$

The **lib** folder contains **module.py** which contains all of the function definitions used in the script **train_embeddings_example.py**. This script learns two-dimensional word embeddings from a very small corpus of less than 200 words. This small corpus contains the phrase "tall buildings" in several places in order to see if the 
