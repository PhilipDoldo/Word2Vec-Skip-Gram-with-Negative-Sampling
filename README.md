# Word2Vec-Skip-Gram-with-Negative-Sampling

This code naively implements Word2Vec word embeddings from scratch using a skip-gram model with negative sampling. Minibatch gradient descent was implemented to train the loss.

The **lib** folder contains **module.py** which contains all of the function definitions used in the script **train_embeddings_example.py**. This script learns two-dimensional word embeddings from a very small corpus of less than 200 words. This small corpus contains the phrase "tall buildings" in several places in order to see if the learned embeddings for "tall" and "buildings" end up close to each other in the embedding space. The plot created by this script indicates that some structure has been learned about the words and we indeed see the words "tall" and "buildings" are very close to each other. This was achieved without any particular effort being put into optimizing the training hyperparameters.  

## Notes on the Skip-Gram Model with Negative Sampling

The data used for training the model is obtained from some written English text which is preprocessed into a list of words, in order as they appear in the written text (puncuation is naively removed and all letters are lowercased). The skip-gram model iterates through each word one-by-one. On each iteration, the selected word is called the *center word* and we consider all words within some prespecified radius about the center word to be the center word's corresponding list of *context words*. We refer to this radius as the *window size*. For example, if the window size is two, then the list of context words consists of the two words to the left of the center word in addition to the two words to the right of the center word (assuming they exist -- existence is only in question for center words sufficiently close to the beginning or end of the list of words). 

For a given center word, in addition to selecting these context words, we also select a list of so-called *negative samples* which are simply words that are not context words for the given center word (i.e. words not within the defined window size of the center word). These negative samples are chosen randomly according to their scaled unigram probabilities based on the data and a predefined scaling parameter. More concretely, letting $\mathcal{W}$ be the set of all words present in the data, the scaled unigram probability of a word $w$ with scaling parameter $\alpha$ is defined as $$P_{\alpha}(w) = \frac{\text{count}(w)^{\alpha}}{\sum_{u \in \mathcal{W}} \text{count}(u)^{\alpha}}$$ where $\text{count}(u)$ is the number of times that the word $u \in \mathcal{W}$ appears in the data. Setting $\alpha < 1$ makes rare words more likely than without scaling and doing so is often done to help improve performance. According to Jurafsky and Martin, the scaling parameter $\alpha$ is often set to $0.75$ in practice and this is the default value used in the code. For every context word, we select $k \in \mathbb{Z}^+$ negative samples. 

From each center word, we can construct a *training point* correspond to each of the center word's context words. Each training point consists of the following words.
- The center word
- A single context word
- $k$ negative samples

Based on what has been written so far, it should *not* be clear where word embeddings come into play or what the purpose of the negative samples is. The main idea is that, given a center word $w \in \mathcal{W}$ and a candidate word $u \in \mathcal{W}$, we effectively want to train a binary classifier that predicts the probability that $u$ is a context word for $w$ (i.e. is within the window about $w$ in the data). If we let $\mathcal{C}_w$ denote the set of context words for the center word $w$, then we are interested in $$P(u \in \mathcal{C}_w | \text{$w$ is the center word})$$. The way we model this probability will be in terms of word embeddings that we will learn. More specifically, for every word $w \in \mathcal{W}$ we define two word embeddings $v_w, c_w \in \mathbb{R}^d$ which are the *center embedding* or $w$ and *context embedding* of $w$, respectively. That is, we have two $d$-dimensional vectors, one of which is the word embedding that represents $w$ when $w$ is being treated as a center word and the other is the word embedding that represents the word $w$ when $w$ is being treated as a context word. These word embeddings are randomly initialized (the code used a standard normal distrbution for each entry) and each of their entries are trained over time to learn the word embeddings, as will be discussed more below. 

Given the word embeddings, we model the context probability as follows. $$P(u \in \mathcal{C}_w | \text{$w$ is the center word}) = \frac{1}{1 + \exp(- v_w^T c_u)} = \sigma \left( v_w^T c_u \right)$$ 

It is worth noting that $$P(u \notin \mathcal{C}_w | \text{$w$ is the center word}) = 1 - P(u \in \mathcal{C}_w | \text{$w$ is the center word}) = 1 - \frac{1}{1 + \exp(- v_w^T c_u)} = \sigma \left(- v_w^T c_u \right).$$ 

Ultimately, we want to choose the word embeddings such that we maximize the probability that all of the actual context words of a given center word are within the context of the center word and all of the corresponding negative samples are not within the context of the center word. This is a probability of an intersection of several events that, in general, we don't know are truly independent. However, we make a very strong simplifying assumption that each of these events are independent and thus for a given training point with center word $w$, context word $u$, and negative samples $y_1, ..., y_k$ we are interested in the following probability $$P( \\{ u \in \mathcal{C}_w \\} \cap \\{ y_1 \notin \mathcal{C}_w \\} \cap \cdots \cap \\{ y_k \notin \mathcal{C}_w \\} | \text{$w$ is the center word} ) = \sigma(v_w^T c_u) \prod\_{i=1}^{k} \sigma(-v_w^T c\_{y_i}).$$

We define the loss $L$ corresponding to this training point to be the negative log of this probability. $$L = - \log(\sigma(v_w^T c_u)) - \sum\_{i=1}^{k} \log(\sigma(-v_w^T c\_{y_i})) $$ Without the log, we are more likely to run into limitations with floating point precision as the product of several probabilities has a higher chance of being zero up to machine precision. Our goal now is to learn word embeddings to minimize this loss using gradient descent, using all of our training points as data to train on.

After updating all of our word embeddings using gradient descent, our final word embedding that we use to represent a word $w \in \mathcal{W}$ is the sum of its center embedding and its context embedding $v_w + c_w$. Choosing to use this sum as the final embedding is ultimately an arbitrary way of combining the information from each of the trained embeddings and other choices exist, but this one is supposedly a common choice. 

It is worth noting that it was important to use the negative samples because otherwise we could have minimized our loss by making $v_w^T c_u$ arbitrarily large (and positive) for every context word and this could result in learning useless embeddings that are all very large (and potentially close to identical) as such embeddings would drive the $\sigma(v_w^T c_u)$ to $1$.
